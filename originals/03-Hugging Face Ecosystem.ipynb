{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92e2b000-2c51-43c0-98e5-14149c7b0fa6",
   "metadata": {},
   "source": [
    "# 3 - Hugging Face Model Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c947ce9-db64-4117-9a90-9ba230cbe88c",
   "metadata": {},
   "source": [
    "<center>\n",
    "    <img src=\"../img/HF-logo-horizontal.png\" width=600>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04547fb6-5719-43b3-a4b7-0de93c11ac91",
   "metadata": {},
   "source": [
    "First, it's important to understand that [Hugging Face https://huggingface.co/](https://huggingface.co/) provides a [vast ecosystem](https://huggingface.co/welcome) for Machine Learning & AI practitioners:\n",
    "\n",
    "* A [repository](https://huggingface.co/models) of ML/AI models\n",
    "* A suite of [software libraries](https://huggingface.co/docs) (mostly in Python and JS)\n",
    "* Over 500k [datasets](https://huggingface.co/datasets) suitable for training or testing\n",
    "* ML/AI [training content](https://huggingface.co/learn)\n",
    "* ML/AI [research publications](https://huggingface.co/papers)\n",
    "* Model hosting UI in [HF Spaces](https://huggingface.co/spaces)\n",
    "* Personal & Organizational [Hugging Face Hub](https://huggingface.co/docs/hub/en/index) to coordiante your use of AI models (local or cloud)\n",
    "* Community [forum](https://discuss.huggingface.co/) for discussion and a [blog](https://huggingface.co/blog)\n",
    "\n",
    "You should definitely explore it more, but for us we'll just focus on the [model repository https://huggingface.co/models](https://huggingface.co/models)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6ba6ab-3d68-477f-9574-3f0279d8bd1c",
   "metadata": {},
   "source": [
    "## 3.1 Find a GGUF model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82258b5-8da5-468e-ad44-e1bbc30ad733",
   "metadata": {},
   "source": [
    "[*GGUF*](https://huggingface.co/docs/hub/en/gguf) (GPT-Generated Unified Format) model files provide a one-file version of a model that can be used with Ollama, LLama.cpp, and other LLM hosting frameworks.  There are other model formats, such as [*Safetensors*](https://huggingface.co/docs/safetensors/en/index), but they require more steps to work with for self-hosting, so we'll stick with GGUF for now.\n",
    "\n",
    "Hugging Face has over [150k GGUF format models](https://huggingface.co/models?library=gguf), so you need to be selective in which ones are suitable for your needs.\n",
    "\n",
    "[https://huggingface.co/models?library=gguf](https://huggingface.co/models?library=gguf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e26051-e75a-4831-a062-cf72aaf347dd",
   "metadata": {},
   "source": [
    "For our purposes, let's look at the [Phi3 Mini 4B parameter model](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf) from Microsoft.\n",
    "\n",
    "You can right away start downloading the Q4 GGUF file which is about 2.4GB, and cross your fingers it downloads while we read through the model card.  You should download this file to the `modelfiles` folder in the tutorial repository.\n",
    "\n",
    "[https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/tree/main](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/tree/main)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27218861-f2b7-43f3-877d-5270849b25ec",
   "metadata": {},
   "source": [
    "## 3.2 Create a `Modelfile` for the GGUF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d2bb7b-cde0-4c63-ba6c-f52f0b938b5f",
   "metadata": {},
   "source": [
    "The *Model Card* for Phi3 Mini specified that the the `Modelfile` can be downloaded with this command:\n",
    "\n",
    "```\n",
    "hf download microsoft/Phi-3-mini-4k-instruct-gguf Modelfile_q4 --local-dir\n",
    "```\n",
    "\n",
    "However if you look in the *\"Files and versions\"* area of the model repository you'll also see you can directly download the file from the web interface.  A version of it is already available in the `model_import` folder in the tutorial repository.\n",
    "\n",
    "`phi3-4b.modelfile`:\n",
    "```\n",
    "FROM ./Phi-3-mini-4k-instruct-q4.gguf\n",
    "\n",
    "TEMPLATE \"\"\"<s>{{ if .Prompt }}<|user|>\n",
    "{{ .Prompt }}<|end|>\n",
    "{{ end }}<|assistant|>\n",
    "{{ .Response }}<|end|>\"\"\"\n",
    "\n",
    "PARAMETER stop <|endoftext|>\n",
    "PARAMETER stop <|assistant|>\n",
    "PARAMETER stop <|end|>\n",
    "\n",
    "PARAMETER num_ctx 4096\n",
    "```\n",
    "\n",
    "Now create the Ollama model from the modelfile:\n",
    "```\n",
    "ollama create phi3-mini:4b -f phi3-4b.modelfile\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4a2689-8663-42df-bc16-d12e86ff4253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460749bc-7a00-473a-9260-ecdd6efdfbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = ollama.generate(model='phi3-mini:4b',\n",
    "                           prompt='What are three things I should do this week?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd07e7e-885e-4403-88b5-8e1bec784a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1389852a-33d4-4410-b47d-6095e9b48b53",
   "metadata": {},
   "source": [
    "## 3.3 Serve GGUF model with Llama.cpp\n",
    "\n",
    "There are several tools you can use for serving local LLMs.  Ollama is a versatile all-in-one framework with UI, CLI, API, and Python libraries that can support both local LLMs and cloud-hosted LLMs, with no constraints on personal or professional/commercial use.\n",
    "\n",
    "However Llama.cpp is a performant and more production-oriented LLM server (and Ollama is built on top of it).\n",
    "\n",
    "```\n",
    "llama-server -m ./Phi-3-mini-4k-instruct-q4.gguf --jinja -c 0 \\\n",
    "             --host 127.0.0.1 --port 8080\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95743eef-c90d-4ff2-8e1d-0645812d8441",
   "metadata": {},
   "source": [
    "Llama.cpp will provide a web interface to the served model(s), in this case available at:\n",
    "\n",
    "[http://127.0.0.1:8080](http://127.0.0.1:8080)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8effd2-97dc-4da7-8e4f-26fa323cc142",
   "metadata": {},
   "source": [
    "## 3.4 Use the `openai` Python library to access the local LLM server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b2f5b5-8ce2-400a-948f-888133970e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf2e855-0e30-4c8c-9349-1fc2e7ffe791",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(base_url=\"http://127.0.0.1:8080/v1\",\n",
    "                # no API key needed when accessing Llama.cpp LLM server\n",
    "                api_key=\"dummy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a54d1-074f-4275-a456-bbedcef0f7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        dict(role=\"system\", content=\"You are a helpful assistant.\"),\n",
    "        dict(role=\"user\",   content=\"What are the top 3 things to know about LLMs?\")\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3414fd13-89b0-4e7a-82e7-7fe8c7019bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = client.chat.completions.create(\n",
    "    model=\"dummy\",  # Llama.cpp can only serve one model at a time, so this is ignored\n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9c794e-26de-4a27-9066-a81828024cd7",
   "metadata": {},
   "source": [
    "The actual response text is buried several layers in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32837a1b-7921-49a0-b726-4b4851fc8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491358c8-a9ad-4ea1-bbc4-6cb10d530c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(dict(role=\"user\", content=\"Please expand on the last point\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e0bf20-a5d9-4530-bdc2-6b1cb028b3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = client.chat.completions.create(\n",
    "    model=\"dummy\",  # Llama.cpp can only serve one model at a time, so this is ignored\n",
    "    messages=messages\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695d3c40-20b8-466b-9e73-9da3ff8f238f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba815fb6-c88a-4c76-b762-c88092b659d2",
   "metadata": {},
   "source": [
    "### EXERCISE: Fetch an LLM from HF and serve it with Ollama and Llama.cpp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d86acba-b65c-45e6-90c3-49715bedf93d",
   "metadata": {},
   "source": [
    "*(10 minutes)*\n",
    "\n",
    "Using the HuggingFace verion of the TinyLlama 1.1B model found here:\n",
    "\n",
    "[https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF](https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF)\n",
    "\n",
    "Select the `Q4_K_M` GGUF variant from the files section.  This indicates:\n",
    "\n",
    "* `Q4` - 4-bit quantization (precision reduction)\n",
    "* `K` - K-means clustering of quantization group weights\n",
    "* `M` - Medium precision quantization (trade-off in size & performance/quality)\n",
    "\n",
    "1. Create a simple 1-line `Modelfile`\n",
    "2. Create the Ollama model\n",
    "3. Run the Ollama model & experiment with a chat session\n",
    "4. Serve the GGUF model through Llama.cpp and connect to it from the Python `openai` library"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
