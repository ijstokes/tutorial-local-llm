{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59c6a83c-4360-4ae1-9c11-08ca41e15fc8",
   "metadata": {},
   "source": [
    "# 2 - Serve Local LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651b3a2c-9b2d-4ec3-87d1-5d0a53b4e5f6",
   "metadata": {},
   "source": [
    "Quick sanity check on the current environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f10044-a5e4-4848-8fa0-d27d5defce9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2d1ccb-f3dd-4234-a7f8-eae2f069e3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638bf2ef-ab9d-4e4c-a5ca-c69ca157698a",
   "metadata": {},
   "source": [
    "## 2.1 Check Ollama available models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e6570b-26ef-4c68-979e-a81d27dcaee4",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../img/ollama-horizontal.png\" alt=\"Ollama logo\">\n",
    "</center>\n",
    "\n",
    "If you did the setup correctly (see [`README.md`](../README.md) in the repo root) then you should see at least a few models already available locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded2a46-3488-4939-a2dd-17b63932f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc8939-9153-4e86-88dd-b4893d8b158c",
   "metadata": {},
   "source": [
    "The model details are a bit buried in the return object from the `.list()` call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d993c5-f454-47e1-9868-8f6a542d1cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = list(ollama.list())[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca52be5-bc18-4b75-aebf-e425ff397379",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nOllama models (local):\\n')\n",
    "for m in models:\n",
    "    print(f'{m.model:<30}\\t'+\n",
    "          f'{m.details.family}\\t'+\n",
    "          f'{m.details.parameter_size}\\t'+\n",
    "          f'{int(m.size/(1e6)):>6} MB\\t'+\n",
    "          f'{m.details.quantization_level}\\t'+\n",
    "          f'{m.details.format}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a014f780-b003-4f6d-bb5b-b539d988a757",
   "metadata": {},
   "source": [
    "## 2.2 Connect to your Ollama local LLM server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72902ab5-c8dd-462c-ba74-bc995ff0b7af",
   "metadata": {},
   "source": [
    "Now make a one-shot request to the smallest LLM, `gemma3:270m`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8dd465-ceac-41f7-aea1-d2afd4bd8efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = ollama.generate(model='gemma3:270m',\n",
    "                           prompt='Tell me a one paragraph story about a chicken')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99ef407-3967-44f1-97f5-ee40589266d0",
   "metadata": {},
   "source": [
    "If that didn't work for you, make sure Ollama is running.  There are two ways to do this:\n",
    "\n",
    "* Desktop native app -- search *Start* (Windows) or *CMD-SPACE* (MacOS) for \"Ollama\" and make sure it is running\n",
    "* From the command line:\n",
    "\n",
    "```bash\n",
    "ollama start\n",
    "```\n",
    "\n",
    "The latter has the advantage that you can see incoming requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6174c583-5375-4ee8-a115-0349fcd21fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dab7ca-034d-4c1f-872a-9d47c282edb3",
   "metadata": {},
   "source": [
    "### EXERCISE: Experiment with different models & one-shot queries\n",
    "*(5 minutes)*\n",
    "\n",
    "Notes:\n",
    "* Start with the smallest model and then increment in parameter size\n",
    "* Use *\"Task Manager\"* (Windows) or *\"Activity Monitor\"* (MacOS) to see how much CPU and RAM Ollama is using\n",
    "* Try the same prompt more than once with the same model to get a sense of intra-model variability\n",
    "* Try the same prompt more than once with different models to get a sense of inter-model variability\n",
    "\n",
    "If the model outputs Markdown, you can display it in a Jupyter notebook with:\n",
    "\n",
    "```python\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(response.response))\n",
    "```\n",
    "\n",
    "Outside of Jupyter notebook you'll need something like [`python-markdown`](https://python-markdown.github.io/) to convert Markdown text to HTML.\n",
    "\n",
    "There is a helper function `printmd()` below that you can use to directly display generated Markdown in Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba05c4-2d39-4a3a-94c7-3bc666f7c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "def printmd(text:str) -> None:\n",
    "    ''' Jupyter-only print function for markdown text '''\n",
    "    display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080e689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "response = ollama.generate(model='gemma2:2b', \n",
    "                           prompt='What are some of the current geo-political issues?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a124e97f-d870-477e-b962-1c33ef9dbe05",
   "metadata": {},
   "outputs": [],
   "source": [
    "printmd(response.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559f47e0-83a0-49ca-a4bf-e4b3543ca865",
   "metadata": {},
   "source": [
    "## 2.3 Chat Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d904452b-83f5-44db-8c65-4f5f45348f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ollama import chat\n",
    "\n",
    "class ChatSession:\n",
    "    def __init__(self,\n",
    "                 model:str,\n",
    "                 system:str = 'You are a helpful chatbot'):\n",
    "        self.model    = model\n",
    "        self.system   = system\n",
    "        self.messages = []\n",
    "\n",
    "        self.messages.append(dict(role='system', content=system))\n",
    "\n",
    "    def prompt(self, msg) -> str:\n",
    "        self.messages.append(dict(role='user', content=msg))\n",
    "        response = chat(model=self.model, messages=self.messages).message.content\n",
    "        self.messages.append(dict(role='assistant', content=response))\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b6afe-739f-48f6-99b6-097675dc1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs = ChatSession(model='gemma2:2b', system='Please provide short and concise answers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c356c25-914f-4731-8ac5-b5226b5ff0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "printmd(cs.prompt(\"I am thinking about a good gift for my mother\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4dcc329-5a19-4ea1-acb6-0b558a35d9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "printmd(cs.prompt(\"I think she'd like a piece of jewelry. Do you have any recommendations?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0092b8ce-fef7-4459-855a-2e3e6c81d62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "printmd(cs.prompt(\"I have a budget of $200, can you just make a suggestion?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb247bf4-f6af-4550-9f4b-ee14c40ceae0",
   "metadata": {},
   "source": [
    "### Record of interaction\n",
    "\n",
    "Our `ChatSession` object has retained a record of the interaction in the `.messages` list attribute.\n",
    "\n",
    "Depending on your objectives you may need to be logging details of chat sessions, including:\n",
    "\n",
    "* model\n",
    "* input\n",
    "* output\n",
    "* performance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2731e8b3-a745-41a4-ac5f-589f10781d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "cs.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751340bf-6ef6-43ad-bf5d-fbdb00010ddd",
   "metadata": {},
   "source": [
    "### EXERCISE: Experiment with your own chat session\n",
    "\n",
    "*(5 minutes)*\n",
    "\n",
    "Use the `ChatSession` object and template above to expirement with your own chat session.\n",
    "\n",
    "Try using a few different models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b76673d-71ff-4da6-9db0-59729abec619",
   "metadata": {},
   "source": [
    "## 2.4 Creating Your Own Models\n",
    "\n",
    "Ollama provides a number of ways to create your own model, from any of these sources:\n",
    "\n",
    "* your local Ollama model repository\n",
    "* the global/public Ollama model repository\n",
    "* GGUF files you have locally\n",
    "\n",
    "This allows you to create model variants to meet your specific needs.  We'll experiment more with this later, but we can start with some basic examples of ephemeral models (i.e. ones that only exist in memory) which use *system prompts* as the basis for creating a model variant.\n",
    "\n",
    "More details on Ollama's `create` API can be found [here](https://github.com/ollama/ollama/blob/main/docs/api.md#create-a-model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c26c72-b64f-43a7-92c3-1839ee90b1b8",
   "metadata": {},
   "source": [
    "### Note on Prompt Engineering\n",
    "\n",
    "Prompt engineering is a critical skill for successfully interacting with LLMs.  Details of how to do this well are out of scope for this tutorial, but as a minimum it is important to understand that *system prompts* provide a universal context for all chat messages within a session.  The model will always consider the system prompt when constructing a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c02b6b-4ab3-4a3e-be63-4127e4b96f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ollama.create(model='mario', \n",
    "              from_='gemma2:2b', \n",
    "              system=\"You are Mario from Super Mario Bros.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6baeb4-eb9b-4542-8465-5b48d140a7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "mario = ollama.generate(model='mario', \n",
    "                        prompt='What is on your mind today?')\n",
    "printmd(mario.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402b0544-7c2c-451a-a55e-a04547e77ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ollama.create(model='sentiment', \n",
    "              from_='gemma2:2b', \n",
    "              system=\"\"\"\n",
    "              You are a sentiment classifier.\n",
    "              Breakdown all inputs by sentences.\n",
    "              Classify each sentence as exactly one of the following:\n",
    "              POSITIVE, NEGATIVE, NEUTRAL, UNCLEAR\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f531e9a5-bf9b-4856-aa7b-a467b7db27da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "classifications = ollama.generate(\n",
    "    model='sentiment', \n",
    "    prompt=\"\"\"\n",
    "    I just finished a long trip.\n",
    "    Visiting Peru was amazing.\n",
    "    I had some great adventures with my brother.\n",
    "    We saw many Incan archeological sites.\n",
    "    My flight home required four flights,\n",
    "    but at least I was able to get home faster than my original itinerary.\n",
    "    I lost my toiletry kit on one of my flights.\"\"\")\n",
    "printmd(classifications.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446cd0be-4bb5-4884-9a92-9e621faf0749",
   "metadata": {},
   "source": [
    "## 2.5 Using `Modelfile` to customize an LLM\n",
    "\n",
    "Ollama has created the `Modelfile` *de facto* standard for defining custom models derived from existing models.\n",
    "\n",
    "[Modelfile Reference](https://docs.ollama.com/modelfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30605e85-cfaf-432b-8085-8ddfbae308bd",
   "metadata": {},
   "source": [
    "`luigi.modelfile`:\n",
    "```\n",
    "FROM    gemma2:2b\n",
    "SYSTEM  \"\"\" You are Luigi from Super Mario Bros.\n",
    "            All responses include some comment\n",
    "            concerning your brother Mario.\n",
    "        \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6830a556-0291-4031-9bf0-b48baa7f1f77",
   "metadata": {},
   "source": [
    "There is not currently a way to use the Python API to process a `Modelfile` to create a new model.\n",
    "\n",
    "You can then use the CLI interface to create your derived model:\n",
    "\n",
    "```\n",
    "ollama create luigi -f modelfiles/luigi.modelfile\n",
    "```\n",
    "\n",
    "and then check to see that it has been created:\n",
    "\n",
    "```\n",
    "ollama list\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92231276-6502-4ddd-99ce-189f970da720",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "luigi = ollama.generate(model='luigi', \n",
    "                        prompt='What is on your mind today?')\n",
    "printmd(luigi.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118569f9-15f0-4027-a49a-09cff1d51afd",
   "metadata": {},
   "source": [
    "### Every Ollama model has an associated `Modelfile` you can inspect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594b03f5-7fc9-484f-b8f0-50bbda05fae5",
   "metadata": {},
   "source": [
    "From the CLI, you can inspect the `Modelfile` of registered Ollama models:\n",
    "\n",
    "```\n",
    "ollama show tinyllama:1.1b --modelfile\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091ba6d6-eebd-4429-a2a1-e5fb1469890b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ollama.show(model='tinyllama:1.1b').model_dump()['modelfile'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812471a0-f79c-4f2a-a6a2-8ad49c492d70",
   "metadata": {},
   "source": [
    "Don't worry too much about the `TEMPLATE` and `PARAMETER stop` portions -- these define the way the model expects to handle the `.System`, `.Prompt`, and `.Response` elements of the model interaction, as defined in the [Modelfile Template reference](https://docs.ollama.com/modelfile#template) and the [Go Template syntax](https://pkg.go.dev/text/template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568aaa1e-723c-4686-9edb-60a810181906",
   "metadata": {},
   "source": [
    "Here's a more sophisticated verison of the Sentiment Classifier model:\n",
    "\n",
    "`sentiment2.modelfile`\n",
    "```\n",
    "FROM        gemma2:2b\n",
    "\n",
    "SYSTEM      \"\"\"\n",
    "            You are a sentiment classifier.\n",
    "            Classify each input as exactly one of the following:\n",
    "            POSITIVE, NEGATIVE, NEUTRAL, UNCLEAR\n",
    "            \"\"\"\n",
    "\n",
    "PARAMETER temperature 0.5\n",
    "PARAMETER num_ctx     1024\n",
    "\n",
    "MESSAGE user        I had a great day\n",
    "MESSAGE assistant   POSITIVE\n",
    "MESSAGE user        That hockey game was insane\n",
    "MESSAGE assistant   UNCLEAR\n",
    "MESSAGE user        We need to go shopping this week\n",
    "MESSAGE assistant   NEUTRAL\n",
    "MESSAGE user        That was one of the worst movies ever\n",
    "MESSAGE assistant   NEGATIVE\n",
    "```\n",
    "\n",
    "Create this model with the following command:\n",
    "\n",
    "```\n",
    "ollama create sentiment2 -f modelfiles/sentiment2.modelfile\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4adbafc-ff1d-41d8-b3d4-44130dcbf934",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = ollama.generate(\n",
    "    model='sentiment2', \n",
    "    prompt=\"We just had a foot of snow - I can't wait to go skiing\")\n",
    "printmd(classification.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05de4b-ce17-4377-aff9-f6d2ff93dbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = ollama.generate(\n",
    "    model='sentiment2', \n",
    "    prompt=\"Revenues were higher than projected\")\n",
    "printmd(classification.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb05175-5ed5-4a53-94f6-e2ffb9d0502b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = ollama.generate(\n",
    "    model='sentiment2', \n",
    "    prompt=\"Supply chain delays led to inventory issues across the network\")\n",
    "printmd(classification.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0df205b-db14-443f-b5b4-bf9870f50719",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification = ollama.generate(\n",
    "    model='sentiment2', \n",
    "    prompt=\"Several key injuries are going to make the next game hard to win\")\n",
    "printmd(classification.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3eadc-e935-4cc1-9796-0e3f11898da4",
   "metadata": {},
   "source": [
    "### Model customization\n",
    "\n",
    "This example shows three different ways the new model is customized:\n",
    "\n",
    "1. Setting `temperature` which affects the degree of randomness.  The range is `(0.0, 1.0)`, where lower is described as *\"more coherent\"* and higher is described as *\"more creative\"*.\n",
    "\n",
    "2. Setting `num_ctx` which is the context window size, measured in *tokens*.  This is a critical parameter for performance & memory consumption.  The default in Ollama for local models is 2048 tokens.  In general a bigger window will result in higher quality output but will increase processing time and RAM consumption.\n",
    "\n",
    "3. Few Shot Learning (FSL) using a short set of example interaction messages between `user` prompts and `assistant` responses.\n",
    "\n",
    "**NOTE1:** What is a *token*?  This is dependent upon the model architecture for how inputs are tokenized, however a rule-of-thumb is that a token represents about 4 Bytes or 4 characters of input text.\n",
    "\n",
    "**NOTE2:** What is a *context window*?  It represents how much \"memory\" the model has to work with, though it is important to consider *Signal-to-Noise* effects of \"too much\" data in memory.  Within an LLM interaction session the total context will grow, up to the maximum context window size, when context then becomes FIFO.\n",
    "\n",
    "Here's a graph from [Meibel regarding LLM context window sizes](https://www.meibel.ai/post/understanding-the-impact-of-increasing-llm-context-windows):\n",
    "\n",
    "<center>\n",
    "<img src=\"../img/meibel-ai-context-window-size-history.png\" width=600>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c65225-d4c5-4d6a-b7df-7eee4c6aca13",
   "metadata": {},
   "source": [
    "### EXERCISE: Create your own custom model using a `Modelfile`\n",
    "\n",
    "*5 minutes*\n",
    "\n",
    "Starting with the `gemma3:2b` model, create a `Modelfile` that will act as a calculator with natural language input.  Construct a system prompt that tells the model how to behave then provide examples of input prompts and output.\n",
    "\n",
    "**NOTE:** Don't be surprised if this is hard to make work -- the base models we're using are not tuned/trained for math.\n",
    "\n",
    "**Challenge:** Get the model to support progressive operations which build on the last output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db84cf6f-2dc2-4ed7-90d7-9ea7caa24cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
